{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "COGS150 Final Project<br>\n",
        "Ben Chen<br>\n",
        "PID: A19062681<br>\n",
        "\n",
        "**Research Question: Are large language models more sensitive to cultural norm violation in that culture's native language compared to English?**\n"
      ],
      "metadata": {
        "id": "ZqLWfD1poCt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "_BNZUTm-oWgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "RGdusIFyoc0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\" Running on device: {device}\")"
      ],
      "metadata": {
        "id": "mFmBa-b0oh-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_prob(model, tokenizer, context, target):\n",
        "    \"\"\"\n",
        "    Calculate the next token probability in an autoregressive way\n",
        "    P(Total) = p1 * p2 * p3 ...\n",
        "    Input:\n",
        "    LLM model, tokenizer, context (prompt), target (next token)\n",
        "    Output:\n",
        "    prob, target_tokens\n",
        "    \"\"\"\n",
        "    model_device = model.device\n",
        "    input_ids = tokenizer.encode(context, return_tensors=\"pt\").to(model_device)\n",
        "    # input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "    target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
        "    target_tokens = tokenizer.convert_ids_to_tokens(target_ids)\n",
        "\n",
        "    log_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t_id in target_ids:\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            # Get output probs\n",
        "            next_token_logits = outputs.logits[0, -1, :] # [batch 0, Last token, All voacb]\n",
        "            next_token_log_probs = torch.nn.functional.log_softmax(next_token_logits, dim=0)\n",
        "\n",
        "            # Store target token log prob\n",
        "            token_log_prob = next_token_log_probs[t_id].item()\n",
        "            log_probs.append(token_log_prob)\n",
        "\n",
        "            # Prepare next step\n",
        "            next_token = torch.tensor([[t_id]]).to(model_device)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    # Sum token logs for the unseen\n",
        "    total_log_prob = sum(log_probs)\n",
        "\n",
        "    return np.exp(total_log_prob), target_tokens\n",
        "\n",
        "def _get_suprisal(total_prob):\n",
        "    \"\"\"\n",
        "    Converts raw probability to bits of surprisal\n",
        "    \"\"\"\n",
        "    if total_prob <= 0: return 0.0 # Avoid math errors\n",
        "    return -np.log2(total_prob)\n",
        "\n",
        "\n",
        "def _save_experiment(df, filename_base=\"COGS150_experiment_results_signal\"):\n",
        "\n",
        "  \"\"\"\n",
        "  Save the LLMology experiment result to cd in Google Drive, with timestamp\n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "  project_folder = \"/content/drive/MyDrive/UCSD_Academics/Fall25/COGS150\"\n",
        "\n",
        "  # Generate file name\n",
        "  timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  full_filename = f\"{filename_base}_{timestamp}.csv\"\n",
        "\n",
        "  save_path = os.path.join(project_folder, full_filename)\n",
        "\n",
        "  try:\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"Experiment results saved to: {save_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error saving experiment results: {e}\")\n"
      ],
      "metadata": {
        "id": "hNZ9kAbZsTvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Models\n",
        "MODEL_NAMES = [\n",
        "    (\"GPT2-XL\", \"gpt2-xl\"),\n",
        "    (\"Qwen-1.5B\", \"Qwen/Qwen2.5-1.5B\"),\n",
        "    (\"BLOOM-1.7B\", \"bigscience/bloom-1b7\"),\n",
        "    (\"pythia-1.4b\", \"EleutherAI/pythia-1.4b\")\n",
        "]"
      ],
      "metadata": {
        "id": "L0_GbWK74Mzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "# Force Mount Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "try:\n",
        "  with open(\"/content/drive/MyDrive/UCSD_Academics/Fall25/COGS150/cultural_stimuli_no_signal.json\", \"r\") as f:\n",
        "    stimuli_data = json.load(f) # stimuli_data a list of dictionaries\n",
        "  print(f\"loaded {len(stimuli_data)} cultural stimuli\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"Error, file not found\")\n",
        "  stimuli_data = []\n",
        "\n",
        "#print(stimuli_data)\n"
      ],
      "metadata": {
        "id": "mVh-62g1ttvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check imported prompt content\n",
        "for trial in stimuli_data:\n",
        "  print(trial[\"prompts\"][\"eng\"][\"context\"])"
      ],
      "metadata": {
        "id": "Z5jmVHx1u7Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Experiment Loop\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_name, hf_path in MODEL_NAMES:\n",
        "\n",
        "  print(f'Running: {model_name}')\n",
        "\n",
        "  try:\n",
        "    # Specify tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "              hf_path,\n",
        "              device_map=\"auto\",\n",
        "              torch_dtype=torch.float16,\n",
        "              trust_remote_code=True\n",
        "          ).eval()\n",
        "\n",
        "\n",
        "    # Run Experiment\n",
        "    print(\"Experiment in Progress\")\n",
        "    for trial in stimuli_data:\n",
        "\n",
        "      # Native Prompt non violation\n",
        "      prob_native_congruent, tokens_native_congruent = _get_prob(\n",
        "          model,\n",
        "          tokenizer,\n",
        "          trial[\"prompts\"][\"native\"][\"context\"],\n",
        "          trial[\"prompts\"][\"native\"][\"target_congruent\"]\n",
        "          )\n",
        "\n",
        "      suprisal_native_congruent = _get_suprisal(\n",
        "          prob_native_congruent\n",
        "          )\n",
        "\n",
        "      # Native Prompt violation\n",
        "      prob_native_violation, tokens_native_violation = _get_prob(\n",
        "          model,\n",
        "          tokenizer,\n",
        "          trial[\"prompts\"][\"native\"][\"context\"],\n",
        "          trial[\"prompts\"][\"native\"][\"target_violation\"]\n",
        "          )\n",
        "\n",
        "      suprisal_native_violation = _get_suprisal(\n",
        "          prob_native_violation\n",
        "          )\n",
        "\n",
        "      # English Prompt non violation\n",
        "      prob_eng_congruent, tokens_eng_congruent = _get_prob(\n",
        "          model,\n",
        "          tokenizer,\n",
        "          trial[\"prompts\"][\"eng\"][\"context\"],\n",
        "          trial[\"prompts\"][\"eng\"][\"target_congruent\"]\n",
        "          )\n",
        "\n",
        "      suprisal_eng_congruent = _get_suprisal(\n",
        "          prob_eng_congruent\n",
        "          )\n",
        "\n",
        "      # English Prompt violation\n",
        "      prob_eng_violation, tokens_eng_violation = _get_prob(\n",
        "          model,\n",
        "          tokenizer,\n",
        "          trial[\"prompts\"][\"eng\"][\"context\"],\n",
        "          trial[\"prompts\"][\"eng\"][\"target_violation\"]\n",
        "          )\n",
        "\n",
        "      suprisal_eng_violation = _get_suprisal(\n",
        "          prob_eng_violation\n",
        "          )\n",
        "\n",
        "      all_results.append({\n",
        "          \"model\": model_name,\n",
        "          \"trial\": trial['norm'],\n",
        "          \"prob_native_congruent\": prob_native_congruent,\n",
        "          \"prob_native_violation\": prob_native_violation,\n",
        "          \"prob_eng_congruent\": prob_eng_congruent,\n",
        "          \"prob_eng_violation\": prob_eng_violation,\n",
        "          \"suprisal_native_congruent\": suprisal_native_congruent,\n",
        "          \"suprisal_native_violation\": suprisal_native_violation,\n",
        "          \"suprisal_eng_congruent\": suprisal_eng_congruent,\n",
        "          \"suprisal_eng_violation\": suprisal_eng_violation,\n",
        "          \"tokens_native_congruent\": tokens_native_congruent,\n",
        "          \"tokens_native_violation\": tokens_native_violation,\n",
        "          \"tokens_eng_congruent\": tokens_eng_congruent,\n",
        "          \"tokens_eng_violation\": tokens_eng_violation\n",
        "      })\n",
        "\n",
        "    # Clean up RAM\n",
        "    print(f\"Finished running {model}\")\n",
        "    print(\"Unloading RAM\")\n",
        "    del model\n",
        "    del tokenizer\n",
        "    gc.collect\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    if torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
        "\n",
        "  except Exception as e:\n",
        "          print(f\" Failed to run {model}: {e}\")\n",
        "\n",
        "df_results = pd.DataFrame(all_results)\n",
        "\n",
        "if 'df_results' in locals() and not df_results.empty:\n",
        "    _save_experiment(df_results, \"experiment_reuslts_no_signal\")\n",
        "else:\n",
        "    print(\"No data to save\")\n"
      ],
      "metadata": {
        "id": "MneP2Knoq76s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Visualizing Check\n",
        "df_results.head()\n",
        "# df_qwen = df_results[df_results[\"model\"] == \"Qwen-1.5B\"]\n",
        "# df_qwen.head()\n",
        "# print((df_qwen[\"suprisal_native_violation\"].sum())-(df_qwen[\"suprisal_native_congruent\"].sum()))\n",
        "# print((df_qwen[\"suprisal_eng_violation\"].sum())-(df_qwen[\"suprisal_eng_congruent\"].sum()))"
      ],
      "metadata": {
        "id": "Vb0Logpav7M5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}